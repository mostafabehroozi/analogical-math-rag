# src/evaluation.py

"""
Evaluation module for the Analogical Reasoning RAG project.

This file provides functions to:
- Evaluate generated answers against ground truths using an LLM.
- Parse and analyze the detailed JSON logs from experiment runs.
- Calculate Pass@K metrics and generate a summary DataFrame.
"""

import logging
import os
import re
import pandas as pd
from tqdm import tqdm
from typing import List, Dict, Any

# Import our custom modules
from src.api_manager import GeminiAPIManager
from src.prompts import create_evaluation_prompt
from src.utils import save_json, save_to_pickle, load_from_pickle

def evaluate_single_answer_with_llm(
    model_answer: str,
    ground_truth: str,
    gemini_manager: GeminiAPIManager,
    config: Dict[str, Any]
) -> bool:
    """
    Evaluates a single model-generated answer against a ground truth using an LLM.

    Args:
        model_answer (str): The answer generated by the pipeline.
        ground_truth (str): The correct answer.
        gemini_manager (GeminiAPIManager): The API manager instance.
        config (Dict): The main configuration dictionary.

    Returns:
        bool: True if the answer is judged as correct, False otherwise.
    """
    logger = logging.getLogger(__name__)
    
    if not model_answer or model_answer.startswith("Error:"):
        return False

    evaluator_model = config['GEMINI_MODEL_NAME_EVALUATOR']
    evaluator_temp = config['DEFAULT_EVALUATOR_TEMPERATURE']
    
    prompt = create_evaluation_prompt(model_answer, ground_truth)
    response = gemini_manager.generate_content(prompt, evaluator_model, evaluator_temp)

    if response['status'] != 'SUCCESS':
        logger.warning(f"LLM-based evaluation failed: {response['error_message']}")
        return False

    raw_text = response['text'].strip()
    print(f"    Evaluator LLM Raw Output: {raw_text}")
    logger.debug(f"Evaluator raw response: '{raw_text}'")
    
    # Use regex to robustly parse the 'true' or 'false' from the structured output
    eval_match = re.search(r"Evaluation:\s*(true|false)", raw_text, re.IGNORECASE)
    
    if eval_match:
        result_str = eval_match.group(1).lower()
        logger.info(f"Parsed evaluation result: {result_str}")
        return result_str == 'true'
    else:
        logger.warning(f"Could not parse 'Evaluation:' line from LLM response. Treating as False.")
        return False

def analyze_experiment_logs(
    all_experiments_logs: Dict[str, List[Dict]],
    ground_truths: List[str],
    gemini_manager: GeminiAPIManager,
    config: Dict[str, Any]
) -> pd.DataFrame:
    """
    Analyzes the complete logs from all experiments to calculate Pass@K metrics.

    Args:
        all_experiments_logs (Dict): A dictionary where keys are experiment names
                                     and values are lists of single-query log dicts.
        ground_truths (List[str]): The list of all ground truth solutions.
        gemini_manager (GeminiAPIManager): The API manager instance.
        config (Dict): The main configuration dictionary.

    Returns:
        pd.DataFrame: A summary DataFrame with Pass@K accuracy for each experiment.
    """
    logger = logging.getLogger(__name__)
    logger.info("Starting analysis of all experiment logs.")
    analysis_summary = []
    
    pass_k_values_to_report = config.get("PASS_K_VALUES_TO_REPORT", [1])
    results_dir = config['RESULTS_DIR']

    for exp_name, query_logs in all_experiments_logs.items():
        logger.info(f"--- Analyzing Experiment: {exp_name} ---")
        if not query_logs:
            logger.warning(f"No logs found for experiment '{exp_name}'. Skipping.")
            continue
        
        # Get config from the first log entry for this experiment
        exp_config = query_logs[0].get("config_flags_used", {})
        n_attempts = exp_config.get("N_PASS_ATTEMPTS", 1)
        
        # Determine which Pass@K values are applicable for this experiment
        pass_k_values = sorted([k for k in pass_k_values_to_report if 0 < k <= n_attempts])
        
        pass_k_counts = {k: 0 for k in pass_k_values}
        total_valid_queries = 0 # Queries with at least one non-error solution attempt
        
        detailed_evaluations = []

        for log in tqdm(query_logs, desc=f"Evaluating {exp_name}"):
            hard_list_idx = log["target_query_original_hard_list_idx"]
            ground_truth = ground_truths[hard_list_idx]
            solution_attempts = log["llm_final_solution_attempts_texts"]

            valid_attempts = [s for s in solution_attempts if isinstance(s, str) and not s.startswith("Error:")]
            if not valid_attempts:
                continue
            
            total_valid_queries += 1
            
            # Evaluate each valid attempt
            is_correct_list = [
                evaluate_single_answer_with_llm(attempt, ground_truth, gemini_manager, config)
                for attempt in valid_attempts
            ]
            
            detailed_evaluations.append({
                "hard_list_idx": hard_list_idx,
                "is_correct_list": is_correct_list,
                "attempts": valid_attempts
            })

            # Check for pass at each k
            for k in pass_k_values:
                # A query passes at k if any of the first k attempts are correct.
                if any(is_correct_list[:k]):
                    pass_k_counts[k] += 1
        
        # Save detailed results for this experiment
        save_json(detailed_evaluations, os.path.join(results_dir, f"{exp_name}_detailed_eval.json"))
        
        # Calculate final metrics
        summary_row = {"experiment_name": exp_name, **exp_config}
        summary_row["total_queries_processed"] = len(query_logs)
        summary_row["total_queries_with_valid_solutions"] = total_valid_queries

        for k in pass_k_values:
            count = pass_k_counts[k]
            accuracy = (count / total_valid_queries) * 100 if total_valid_queries > 0 else 0
            summary_row[f"pass@{k}_count"] = count
            summary_row[f"pass@{k}_accuracy (%)"] = round(accuracy, 2)
        
        analysis_summary.append(summary_row)
        
    return pd.DataFrame(analysis_summary)
